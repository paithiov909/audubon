[{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"Apache License","title":"Apache License","text":"Version 2.0, January 2004 <http://www.apache.org/licenses/>","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_1-definitions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"1. Definitions","title":"Apache License","text":"“License” shall mean terms conditions use, reproduction, distribution defined Sections 1 9 document. “Licensor” shall mean copyright owner entity authorized copyright owner granting License. “Legal Entity” shall mean union acting entity entities control, controlled , common control entity. purposes definition, “control” means () power, direct indirect, cause direction management entity, whether contract otherwise, (ii) ownership fifty percent (50%) outstanding shares, (iii) beneficial ownership entity. “” (“”) shall mean individual Legal Entity exercising permissions granted License. “Source” form shall mean preferred form making modifications, including limited software source code, documentation source, configuration files. “Object” form shall mean form resulting mechanical transformation translation Source form, including limited compiled object code, generated documentation, conversions media types. “Work” shall mean work authorship, whether Source Object form, made available License, indicated copyright notice included attached work (example provided Appendix ). “Derivative Works” shall mean work, whether Source Object form, based (derived ) Work editorial revisions, annotations, elaborations, modifications represent, whole, original work authorship. purposes License, Derivative Works shall include works remain separable , merely link (bind name) interfaces , Work Derivative Works thereof. “Contribution” shall mean work authorship, including original version Work modifications additions Work Derivative Works thereof, intentionally submitted Licensor inclusion Work copyright owner individual Legal Entity authorized submit behalf copyright owner. purposes definition, “submitted” means form electronic, verbal, written communication sent Licensor representatives, including limited communication electronic mailing lists, source code control systems, issue tracking systems managed , behalf , Licensor purpose discussing improving Work, excluding communication conspicuously marked otherwise designated writing copyright owner “Contribution.” “Contributor” shall mean Licensor individual Legal Entity behalf Contribution received Licensor subsequently incorporated within Work.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_2-grant-of-copyright-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"2. Grant of Copyright License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable copyright license reproduce, prepare Derivative Works , publicly display, publicly perform, sublicense, distribute Work Derivative Works Source Object form.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_3-grant-of-patent-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"3. Grant of Patent License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable (except stated section) patent license make, made, use, offer sell, sell, import, otherwise transfer Work, license applies patent claims licensable Contributor necessarily infringed Contribution(s) alone combination Contribution(s) Work Contribution(s) submitted. institute patent litigation entity (including cross-claim counterclaim lawsuit) alleging Work Contribution incorporated within Work constitutes direct contributory patent infringement, patent licenses granted License Work shall terminate date litigation filed.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_4-redistribution","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"4. Redistribution","title":"Apache License","text":"may reproduce distribute copies Work Derivative Works thereof medium, without modifications, Source Object form, provided meet following conditions: () must give recipients Work Derivative Works copy License; (b) must cause modified files carry prominent notices stating changed files; (c) must retain, Source form Derivative Works distribute, copyright, patent, trademark, attribution notices Source form Work, excluding notices pertain part Derivative Works; (d) Work includes “NOTICE” text file part distribution, Derivative Works distribute must include readable copy attribution notices contained within NOTICE file, excluding notices pertain part Derivative Works, least one following places: within NOTICE text file distributed part Derivative Works; within Source form documentation, provided along Derivative Works; , within display generated Derivative Works, wherever third-party notices normally appear. contents NOTICE file informational purposes modify License. may add attribution notices within Derivative Works distribute, alongside addendum NOTICE text Work, provided additional attribution notices construed modifying License. may add copyright statement modifications may provide additional different license terms conditions use, reproduction, distribution modifications, Derivative Works whole, provided use, reproduction, distribution Work otherwise complies conditions stated License.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_5-submission-of-contributions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"5. Submission of Contributions","title":"Apache License","text":"Unless explicitly state otherwise, Contribution intentionally submitted inclusion Work Licensor shall terms conditions License, without additional terms conditions. Notwithstanding , nothing herein shall supersede modify terms separate license agreement may executed Licensor regarding Contributions.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_6-trademarks","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"6. Trademarks","title":"Apache License","text":"License grant permission use trade names, trademarks, service marks, product names Licensor, except required reasonable customary use describing origin Work reproducing content NOTICE file.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_7-disclaimer-of-warranty","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"7. Disclaimer of Warranty","title":"Apache License","text":"Unless required applicable law agreed writing, Licensor provides Work (Contributor provides Contributions) “” BASIS, WITHOUT WARRANTIES CONDITIONS KIND, either express implied, including, without limitation, warranties conditions TITLE, NON-INFRINGEMENT, MERCHANTABILITY, FITNESS PARTICULAR PURPOSE. solely responsible determining appropriateness using redistributing Work assume risks associated exercise permissions License.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_8-limitation-of-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"8. Limitation of Liability","title":"Apache License","text":"event legal theory, whether tort (including negligence), contract, otherwise, unless required applicable law (deliberate grossly negligent acts) agreed writing, shall Contributor liable damages, including direct, indirect, special, incidental, consequential damages character arising result License use inability use Work (including limited damages loss goodwill, work stoppage, computer failure malfunction, commercial damages losses), even Contributor advised possibility damages.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"id_9-accepting-warranty-or-additional-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"9. Accepting Warranty or Additional Liability","title":"Apache License","text":"redistributing Work Derivative Works thereof, may choose offer, charge fee , acceptance support, warranty, indemnity, liability obligations /rights consistent License. However, accepting obligations, may act behalf sole responsibility, behalf Contributor, agree indemnify, defend, hold Contributor harmless liability incurred , claims asserted , Contributor reason accepting warranty additional liability. END TERMS CONDITIONS","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":"appendix-how-to-apply-the-apache-license-to-your-work","dir":"","previous_headings":"","what":"APPENDIX: How to apply the Apache License to your work","title":"Apache License","text":"apply Apache License work, attach following boilerplate notice, fields enclosed brackets [] replaced identifying information. (Don’t include brackets!) text enclosed appropriate comment syntax file format. also recommend file class name description purpose included “printed page” copyright notice easier identification within third-party archives.","code":"Copyright [yyyy] [name of copyright owner]  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."},{"path":"https://paithiov909.github.io/audubon/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Akiru Kato. Maintainer, author. Koki Takahashi. Copyright holder.            Author japanese.js Shuhei Iitsuka. Copyright holder.            Author budoux Taku Kudo. Copyright holder.            Author TinySegmenter","code":""},{"path":"https://paithiov909.github.io/audubon/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kato (2022). audubon: Japanese Text Processing Tools. https://github.com/paithiov909/audubon, https://paithiov909.github.io/audubon/.","code":"@Manual{,   title = {audubon: Japanese Text Processing Tools},   author = {Akiru Kato},   year = {2022},   note = {https://github.com/paithiov909/audubon, https://paithiov909.github.io/audubon/}, }"},{"path":"https://paithiov909.github.io/audubon/index.html","id":"audubon-","dir":"","previous_headings":"","what":"Japanese Text Processing Tools","title":"Japanese Text Processing Tools","text":"audubon Japanese text processing tools : filling Japanese iteration marks hiraganization, katakanization romanization using hakatashi/japanese.js segmentation phrase using google/budoux ‘TinySegmenter.js’ text normalization based rules ‘Sudachi’ morphological analyzer ‘NEologd’ (Neologism dictionary ‘MeCab’). features implemented ‘ICU’ (.e., stringi package), goal audubon package provide additional features.","code":""},{"path":"https://paithiov909.github.io/audubon/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Japanese Text Processing Tools","text":"","code":"remotes::install_github(\"paithiov909/audubon\")"},{"path":[]},{"path":"https://paithiov909.github.io/audubon/index.html","id":"fill-japanese-iteration-marks-odori-ji","dir":"","previous_headings":"Usage","what":"Fill Japanese iteration marks (Odori-ji)","title":"Japanese Text Processing Tools","text":"strj_fill_iter_mark repeats previous character replaces iteration marks element 5 characters. can use feature strj_normalize strj_rewrite_as_def.","code":"strj_fill_iter_mark(c(   \"あいうゝ〃かき\",   \"金子みすゞ\",   \"のたり〳〵かな\",   \"しろ／″＼とした\" )) #> [1] \"あいうううかき\"  \"金子みすすﾞ\"     \"のたりたりかな\"  \"しろしﾞろとした\"  strj_fill_iter_mark(\"いすゞエルフトラック\") |>   strj_normalize() #> [1] \"いすずエルフトラック\""},{"path":"https://paithiov909.github.io/audubon/index.html","id":"character-class-conversion","dir":"","previous_headings":"Usage","what":"Character class conversion","title":"Japanese Text Processing Tools","text":"Character class conversion uses hakatashi/japanese.js.","code":"strj_hiraganize(\"あのイーハトーヴォのすきとおった風\") #> [1] \"あのいーはとーゔぉのすきとおった風\" strj_katakanize(\"あのイーハトーヴォのすきとおった風\") #> [1] \"アノイーハトーヴォノスキトオッタ風\" strj_romanize(\"あのイーハトーヴォのすきとおった風\") #> [1] \"anoīhatōvonosukitōtta\""},{"path":"https://paithiov909.github.io/audubon/index.html","id":"segmentation-by-phrase","dir":"","previous_headings":"Usage","what":"Segmentation by phrase","title":"Japanese Text Processing Tools","text":"strj_tokenize splits Japanese text phrases using google/budoux, TinySegmenter, tokenizers.","code":"strj_tokenize(\"あのイーハトーヴォのすきとおった風\", engine = \"budoux\") #> $`1` #> [1] \"あのイーハトーヴォの\" \"すきと\"               \"おった\"               #> [4] \"風\""},{"path":"https://paithiov909.github.io/audubon/index.html","id":"japanese-text-normalization","dir":"","previous_headings":"Usage","what":"Japanese text normalization","title":"Japanese Text Processing Tools","text":"strj_normalize normalizes text following rule based NEologd style. strj_rewrite_as_def R port SudachiCharNormalizer typically normalizes characters following ’*.def’ file. audubon package contains several ’*.def’ files, can use write ‘rewrite.def’ file follows. feature powerful stringi::stri_trans_* allows users control characters normalized. instance, function can used convert kyuji-tai characters shinji-tai characters.","code":"strj_normalize(\"――南アルプスの　天然水-　Ｓｐａｒｋｉｎｇ*　Ｌｅｍｏｎ+　レモン一絞り\") #> [1] \"ー南アルプスの天然水-Sparking* Lemon+レモン一絞り\" # single characters will **never** be normalized. … # if two characters are separated with a tab, # left side forms are always rewritten to right side forms # before normalized. 斎   斉 齋   斉 齊   斉 # supports rewriting a single character to a single character, # i.e., this cannot work. ｱｯ  ア stringi::stri_trans_nfkc(\"Ⅹⅳ\") #> [1] \"Xiv\" strj_rewrite_as_def(\"Ⅹⅳ\") #> [1] \"Ⅹⅳ\" strj_rewrite_as_def(\"惡と假面のルール\", read_rewrite_def(system.file(\"def/kyuji.def\", package = \"audubon\"))) #> [1] \"悪と仮面のルール\""},{"path":"https://paithiov909.github.io/audubon/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Japanese Text Processing Tools","text":"© 2022 Akiru Kato Licensed Apache License, Version 2.0. Icons made iconixar www.flaticon.com.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/audubon-package.html","id":null,"dir":"Reference","previous_headings":"","what":"audubon: Japanese Text Processing Tools — audubon-package","title":"audubon: Japanese Text Processing Tools — audubon-package","text":"collection Japanese text processing tools filling Japanese iteration marks, Japanese character type conversions, segmentation phrase, text normalization based rules 'Sudachi' morphological analyzer 'NEologd' (Neologism dictionary 'MeCab'). features specific Japanese implemented 'ICU' (International Components Unicode).","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/audubon-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"audubon: Japanese Text Processing Tools — audubon-package","text":"Maintainer: Akiru Kato paithiov909@gmail.com contributors: Koki Takahashi (Author japanese.js) [copyright holder] Shuhei Iitsuka (Author budoux) [copyright holder] Taku Kudo (Author TinySegmenter) [copyright holder]","code":""},{"path":"https://paithiov909.github.io/audubon/reference/bind_tf_idf2.html","id":null,"dir":"Reference","previous_headings":"","what":"Bind the term frequency and inverse document frequency — bind_tf_idf2","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"Calculates binds term frequency, inverse document frequency, TF-IDF dataset. function experimentally supports 3 types term frequencies 4 types inverse document frequencies, implemented 'RMeCab' package.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/bind_tf_idf2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"","code":"bind_tf_idf2(   tbl,   term = \"token\",   document = \"doc_id\",   n = \"n\",   tf = c(\"tf\", \"tf2\", \"tf3\"),   idf = c(\"idf\", \"idf2\", \"idf3\", \"idf4\"),   norm = FALSE,   rmecab_compat = TRUE )"},{"path":"https://paithiov909.github.io/audubon/reference/bind_tf_idf2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"tbl tidy text dataset. term Column containing terms string symbol. document Column containing document IDs string symbol. n Column containing document-term counts string symbol. tf Method computing term frequency. idf Method computing inverse document frequency. norm Logical; supplied TRUE, raw term counts normalized divided L2 norms computing IDF values. rmecab_compat Logical; supplied TRUE, computes values taking care compatibility 'RMeCab'. Note 'RMeCab' always computes IDF values using term frequency rather raw term counts, thus TF-IDF values may doubly affected term frequency.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/bind_tf_idf2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/bind_tf_idf2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"Types term frequency can switched tf argument: tf term frequency (raw count terms). tf2 logarithmic term frequency base 10. tf3 binary-weighted term frequency. Types inverse document frequencies can switched idf argument: idf inverse document frequency base 2, smoothed. 'smoothed' means just adding 1 raw counts logarithmizing. idf2 global frequency IDF. idf3 probabilistic IDF base 2. idf4 global entropy, IDF actual.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/bind_tf_idf2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bind the term frequency and inverse document frequency — bind_tf_idf2","text":"","code":"if (FALSE) { df <- dplyr::group_by(hiroba, doc_id) |>   dplyr::count(token) |>   dplyr::ungroup() bind_tf_idf2(df) }"},{"path":"https://paithiov909.github.io/audubon/reference/collapse_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Collapse sequences of tokens by condition — collapse_tokens","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"Concatenates sequences tokens tidy text dataset, grouping expression.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/collapse_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"","code":"collapse_tokens(tbl, condition, .collapse = \"\")"},{"path":"https://paithiov909.github.io/audubon/reference/collapse_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"tbl tidy text dataset. condition logical expression. .collapse String tokens concatenated.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/collapse_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/collapse_tokens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"Note function drops columns except 'token' columns grouping sequences. , returned data.frame 'doc_id', 'sentence_id', 'token_id', 'token' columns.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/collapse_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collapse sequences of tokens by condition — collapse_tokens","text":"","code":"df <- prettify(head(hiroba), col_select = \"POS1\") collapse_tokens(df, POS1 == \"\\u540d\\u8a5e\") #> # A tibble: 5 × 4 #>   doc_id sentence_id token_id token    #>   <fct>        <int>    <int> <chr>    #> 1 1                1        1 ポラーノ #> 2 1                1        2 の       #> 3 1                1        3 広場     #> 4 2                2        1 宮沢賢治 #> 5 3                3        1 前"},{"path":"https://paithiov909.github.io/audubon/reference/get_dict_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dictionary's features — get_dict_features","title":"Get dictionary's features — get_dict_features","text":"Returns dictionary's features. Currently supports \"unidic17\" (2.1.2 src schema), \"unidic26\" (2.1.2 bin schema), \"unidic29\" (schema used 2.2.0, 2.3.0), \"cc-cedict\", \"ko-dic\" (mecab-ko-dic), \"naist11\", \"sudachi\", \"ipa\".","code":""},{"path":"https://paithiov909.github.io/audubon/reference/get_dict_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dictionary's features — get_dict_features","text":"","code":"get_dict_features(   dict = c(\"ipa\", \"unidic17\", \"unidic26\", \"unidic29\", \"cc-cedict\", \"ko-dic\", \"naist11\",     \"sudachi\") )"},{"path":"https://paithiov909.github.io/audubon/reference/get_dict_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dictionary's features — get_dict_features","text":"dict Character scalar; one \"ipa\", \"unidic17\", \"unidic26\", \"unidic29\", \"cc-cedict\", \"ko-dic\", \"naist11\", \"sudachi\".","code":""},{"path":"https://paithiov909.github.io/audubon/reference/get_dict_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dictionary's features — get_dict_features","text":"character vector.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/get_dict_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get dictionary's features — get_dict_features","text":"","code":"get_dict_features(\"ipa\") #> [1] \"POS1\"        \"POS2\"        \"POS3\"        \"POS4\"        \"X5StageUse1\" #> [6] \"X5StageUse2\" \"Original\"    \"Yomi1\"       \"Yomi2\""},{"path":"https://paithiov909.github.io/audubon/reference/hiroba.html","id":null,"dir":"Reference","previous_headings":"","what":"Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — hiroba","title":"Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — hiroba","text":"tidy text data audubon::polano tokenized 'MeCab'.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/hiroba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — hiroba","text":"","code":"hiroba"},{"path":"https://paithiov909.github.io/audubon/reference/hiroba.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — hiroba","text":"object class data.frame 26849 rows 5 columns.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/hiroba.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Whole tokens of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — hiroba","text":"","code":"head(hiroba) #>   doc_id sentence_id token_id    token #> 1      1           1        1 ポラーノ #> 2      1           1        2       の #> 3      1           1        3     広場 #> 4      2           2        1     宮沢 #> 5      2           2        2     賢治 #> 6      3           3        1       前 #>                                            feature #> 1                              名詞,一般,*,*,*,*,* #> 2                     助詞,連体化,*,*,*,*,の,ノ,ノ #> 3             名詞,一般,*,*,*,*,広場,ヒロバ,ヒロバ #> 4 名詞,固有名詞,人名,姓,*,*,宮沢,ミヤザワ,ミヤザワ #> 5     名詞,固有名詞,人名,名,*,*,賢治,ケンジ,ケンジ #> 6             接頭詞,名詞接続,*,*,*,*,前,ゼン,ゼン"},{"path":"https://paithiov909.github.io/audubon/reference/lex_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate lexical density — lex_density","title":"Calculate lexical density — lex_density","text":"lexical density proportion content words (lexical items) documents. function simple helper calculating lexical density given datasets.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/lex_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate lexical density — lex_density","text":"","code":"lex_density(vec, contents_words, targets = NULL, negate = c(FALSE, FALSE))"},{"path":"https://paithiov909.github.io/audubon/reference/lex_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate lexical density — lex_density","text":"vec character vector. contents_words character vector containing values counted contents words. targets character vector denominator lexical density filtered computing values. negate logical vector length 2. supplied TRUE, respectively negates predicate functions counting contents words targets.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/lex_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate lexical density — lex_density","text":"numeric vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/lex_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate lexical density — lex_density","text":"","code":"head(hiroba) |>   prettify(col_select = \"POS1\") |>   dplyr::group_by(doc_id) |>   dplyr::summarise(     noun_ratio = lex_density(POS1,       \"\\u540d\\u8a5e\",       c(\"\\u52a9\\u8a5e\", \"\\u52a9\\u52d5\\u8a5e\"),       negate = c(FALSE, TRUE)     ),     mvr = lex_density(       POS1,       c(\"\\u5f62\\u5bb9\\u8a5e\", \"\\u526f\\u8a5e\", \"\\u9023\\u4f53\\u8a5e\"),       \"\\u52d5\\u8a5e\"     ),     vnr = lex_density(POS1, \"\\u52d5\\u8a5e\", \"\\u540d\\u8a5e\")   ) #> # A tibble: 3 × 4 #>   doc_id noun_ratio   mvr   vnr #>   <fct>       <dbl> <dbl> <dbl> #> 1 1               1   NaN     0 #> 2 2               1   NaN     0 #> 3 3               0   NaN   NaN"},{"path":"https://paithiov909.github.io/audubon/reference/mute_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Mute tokens by condition — mute_tokens","title":"Mute tokens by condition — mute_tokens","text":"Permutes tokens tidy text dataset string scalar matched expression.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/mute_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mute tokens by condition — mute_tokens","text":"","code":"mute_tokens(tbl, condition, .as = NA_character_)"},{"path":"https://paithiov909.github.io/audubon/reference/mute_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mute tokens by condition — mute_tokens","text":"tbl tidy text dataset. condition logical expression. .String tokens replaced matched condition. default value NA_character.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/mute_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mute tokens by condition — mute_tokens","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/mute_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mute tokens by condition — mute_tokens","text":"","code":"df <- prettify(head(hiroba), col_select = \"POS1\") mute_tokens(df, POS1 %in% c(\"\\u52a9\\u8a5e\", \"\\u52a9\\u52d5\\u8a5e\")) #>   doc_id sentence_id token_id    token   POS1 #> 1      1           1        1 ポラーノ   名詞 #> 2      1           1        2     <NA>   助詞 #> 3      1           1        3     広場   名詞 #> 4      2           2        1     宮沢   名詞 #> 5      2           2        2     賢治   名詞 #> 6      3           3        1       前 接頭詞"},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Ngrams tokenizer — ngram_tokenizer","title":"Ngrams tokenizer — ngram_tokenizer","text":"Make ngram tokenizer function.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ngrams tokenizer — ngram_tokenizer","text":"","code":"ngram_tokenizer(n = 1L)"},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ngrams tokenizer — ngram_tokenizer","text":"n Integer.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ngrams tokenizer — ngram_tokenizer","text":"ngram tokenizer function","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":null,"dir":"Reference","previous_headings":"","what":"Pack prettified data.frame of tokens — pack","title":"Pack prettified data.frame of tokens — pack","text":"Packs prettified data.frame tokens new data.frame corpus, compatible Text Interchange Formats.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pack prettified data.frame of tokens — pack","text":"","code":"pack(tbl, pull = \"token\", n = 1L, sep = \"-\", .collapse = \" \")"},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pack prettified data.frame of tokens — pack","text":"tbl prettified data.frame tokens. pull Column packed text ngrams body. Default value token. n Integer internally passed ngrams tokenizer function created audubon::ngram_tokenizer() sep Character scalar internally used concatenator ngrams. .collapse argument passed stringi::stri_join().","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pack prettified data.frame of tokens — pack","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"text-interchange-formats-tif-","dir":"Reference","previous_headings":"","what":"Text Interchange Formats (TIF)","title":"Pack prettified data.frame of tokens — pack","text":"Text Interchange Formats (TIF) set standards allows R text analysis packages target defined inputs outputs corpora, tokens, document-term matrices.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"valid-data-frame-of-tokens","dir":"Reference","previous_headings":"","what":"Valid data.frame of tokens","title":"Pack prettified data.frame of tokens — pack","text":"prettified data.frame tokens data.frame object compatible TIF. TIF valid data.frame tokens expected one unique key column (named doc_id) text several feature columns tokens. feature columns must contain least token .","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pack prettified data.frame of tokens — pack","text":"","code":"pack(strj_tokenize(polano[1:5], format = \"data.frame\")) #>   doc_id #> 1      1 #> 2      2 #> 3      3 #> 4      4 #> 5      5 #>                                                                          text #> 1                                                            ポラーノ の 広場 #> 2                                                                   宮沢 賢治 #> 3                                  前 十七 等 官 レ オー ノ ・ キュー スト 誌 #> 4                                                              宮沢 賢治 訳述 #> 5 その ころ わたくし は 、 モリーオ 市 の 博物 局 に 勤め て 居 り ま した 。"},{"path":"https://paithiov909.github.io/audubon/reference/polano.html","id":null,"dir":"Reference","previous_headings":"","what":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","title":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","text":"Whole text 'Porano Hiroba' written Miyazawa Kenji Aozora Bunko","code":""},{"path":"https://paithiov909.github.io/audubon/reference/polano.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","text":"","code":"polano"},{"path":"https://paithiov909.github.io/audubon/reference/polano.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","text":"object class character length 899.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/polano.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","text":"https://www.aozora.gr.jp/cards/000081/files/1935_ruby_19924.zip","code":""},{"path":"https://paithiov909.github.io/audubon/reference/polano.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","text":"dataset containing text Miyazawa Kenji's novel \"Porano Hiroba\" published 1934, year Kenji's death. Copyright work expired since 70 years passed author's death. UTF-8 plain text sourced https://www.aozora.gr.jp/cards/000081/card1935.html cleaned meta data.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/polano.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Whole text of 'Porano no Hiroba' written by Miyazawa Kenji\nfrom Aozora Bunko — polano","text":"","code":"head(polano) #> [1] \"ポラーノの広場\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           #> [2] \"宮沢賢治\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 #> [3] \"前十七等官レオーノ・キュースト誌\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         #> [4] \"宮沢賢治訳述\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             #> [5] \"そのころわたくしは、モリーオ市の博物局に勤めて居りました。\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               #> [6] \"十八等官でしたから役所のなかでも、ずうっと下の方でしたし俸給もほんのわずかでしたが、受持ちが標本の採集や整理で生れ付き好きなことでしたから、わたくしは毎日ずいぶん愉快にはたらきました。殊にそのころ、モリーオ市では競馬場を植物園に拵え直すというので、その景色のいいまわりにアカシヤを植え込んだ広い地面が、切符売場や信号所の建物のついたまま、わたくしどもの役所の方へまわって来たものですから、わたくしはすぐ宿直という名前で月賦で買った小さな蓄音器と二十枚ばかりのレコードをもって、その番小屋にひとり住むことになりました。わたくしはそこの馬を置く場所に板で小さなしきいをつけて一疋の山羊を飼いました。毎朝その乳をしぼってつめたいパンをひたしてたべ、それから黒い革のかばんへすこしの書類や雑誌を入れ、靴もきれいにみがき、並木のポプラの影法師を大股にわたって市の役所へ出て行くのでした。\""},{"path":"https://paithiov909.github.io/audubon/reference/prettify.html","id":null,"dir":"Reference","previous_headings":"","what":"Prettify tokenized output — prettify","title":"Prettify tokenized output — prettify","text":"Turns single character column features separating delimiter.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/prettify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prettify tokenized output — prettify","text":"","code":"prettify(   df,   col = \"feature\",   into = get_dict_features(\"ipa\"),   col_select = seq_along(into),   delim = \",\" )"},{"path":"https://paithiov909.github.io/audubon/reference/prettify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prettify tokenized output — prettify","text":"df data.frame feature column prettified. col Column name prettified. Character vector used column names features. col_select Character integer vector kept prettified features. delim Character scalar used separate fields within feature.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/prettify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prettify tokenized output — prettify","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/prettify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prettify tokenized output — prettify","text":"","code":"prettify(   data.frame(x = c(\"x,y\", \"y,z\", \"z,x\")),   col = \"x\",   into = c(\"a\", \"b\"),   col_select = \"b\" ) #>   b #> 1 y #> 2 z #> 3 x"},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a rewrite.def file — read_rewrite_def","title":"Read a rewrite.def file — read_rewrite_def","text":"Read rewrite.def file","code":""},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a rewrite.def file — read_rewrite_def","text":"","code":"read_rewrite_def(   def_path = system.file(\"def/rewrite.def\", package = \"audubon\") )"},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a rewrite.def file — read_rewrite_def","text":"def_path Character scalar; path rewriting definition file.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a rewrite.def file — read_rewrite_def","text":"list.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a rewrite.def file — read_rewrite_def","text":"","code":"str(read_rewrite_def()) #> List of 2 #>  $ ignore :List of 836 #>   ..$ : chr \"Ⅰ\" #>   ..$ : chr \"Ⅱ\" #>   ..$ : chr \"Ⅲ\" #>   ..$ : chr \"Ⅳ\" #>   ..$ : chr \"Ⅴ\" #>   ..$ : chr \"Ⅵ\" #>   ..$ : chr \"Ⅶ\" #>   ..$ : chr \"Ⅷ\" #>   ..$ : chr \"Ⅸ\" #>   ..$ : chr \"Ⅹ\" #>   ..$ : chr \"Ⅺ\" #>   ..$ : chr \"Ⅻ\" #>   ..$ : chr \"Ⅼ\" #>   ..$ : chr \"Ⅽ\" #>   ..$ : chr \"Ⅾ\" #>   ..$ : chr \"Ⅿ\" #>   ..$ : chr \"ⅰ\" #>   ..$ : chr \"ⅱ\" #>   ..$ : chr \"ⅲ\" #>   ..$ : chr \"ⅳ\" #>   ..$ : chr \"ⅴ\" #>   ..$ : chr \"ⅵ\" #>   ..$ : chr \"ⅶ\" #>   ..$ : chr \"ⅷ\" #>   ..$ : chr \"ⅸ\" #>   ..$ : chr \"ⅹ\" #>   ..$ : chr \"ⅺ\" #>   ..$ : chr \"ⅻ\" #>   ..$ : chr \"ⅼ\" #>   ..$ : chr \"ⅽ\" #>   ..$ : chr \"ⅾ\" #>   ..$ : chr \"ⅿ\" #>   ..$ : chr \"⺀\" #>   ..$ : chr \"⺁\" #>   ..$ : chr \"⺂\" #>   ..$ : chr \"⺃\" #>   ..$ : chr \"⺄\" #>   ..$ : chr \"⺅\" #>   ..$ : chr \"⺆\" #>   ..$ : chr \"⺇\" #>   ..$ : chr \"⺈\" #>   ..$ : chr \"⺉\" #>   ..$ : chr \"⺊\" #>   ..$ : chr \"⺋\" #>   ..$ : chr \"⺌\" #>   ..$ : chr \"⺍\" #>   ..$ : chr \"⺎\" #>   ..$ : chr \"⺏\" #>   ..$ : chr \"⺐\" #>   ..$ : chr \"⺑\" #>   ..$ : chr \"⺒\" #>   ..$ : chr \"⺓\" #>   ..$ : chr \"⺔\" #>   ..$ : chr \"⺕\" #>   ..$ : chr \"⺖\" #>   ..$ : chr \"⺗\" #>   ..$ : chr \"⺘\" #>   ..$ : chr \"⺙\" #>   ..$ : chr \"⺛\" #>   ..$ : chr \"⺜\" #>   ..$ : chr \"⺝\" #>   ..$ : chr \"⺞\" #>   ..$ : chr \"⺟\" #>   ..$ : chr \"⺠\" #>   ..$ : chr \"⺡\" #>   ..$ : chr \"⺢\" #>   ..$ : chr \"⺣\" #>   ..$ : chr \"⺤\" #>   ..$ : chr \"⺥\" #>   ..$ : chr \"⺦\" #>   ..$ : chr \"⺧\" #>   ..$ : chr \"⺨\" #>   ..$ : chr \"⺩\" #>   ..$ : chr \"⺪\" #>   ..$ : chr \"⺫\" #>   ..$ : chr \"⺬\" #>   ..$ : chr \"⺭\" #>   ..$ : chr \"⺮\" #>   ..$ : chr \"⺯\" #>   ..$ : chr \"⺰\" #>   ..$ : chr \"⺱\" #>   ..$ : chr \"⺲\" #>   ..$ : chr \"⺳\" #>   ..$ : chr \"⺴\" #>   ..$ : chr \"⺵\" #>   ..$ : chr \"⺶\" #>   ..$ : chr \"⺷\" #>   ..$ : chr \"⺸\" #>   ..$ : chr \"⺹\" #>   ..$ : chr \"⺺\" #>   ..$ : chr \"⺻\" #>   ..$ : chr \"⺼\" #>   ..$ : chr \"⺽\" #>   ..$ : chr \"⺾\" #>   ..$ : chr \"⺿\" #>   ..$ : chr \"⻀\" #>   ..$ : chr \"⻁\" #>   ..$ : chr \"⻂\" #>   ..$ : chr \"⻃\" #>   .. [list output truncated] #>  $ replace:List of 182 #>   ..$ : chr [1:2] \"ｳﾞ\" \"ヴ\" #>   ..$ : chr [1:2] \"ｶﾞ\" \"ガ\" #>   ..$ : chr [1:2] \"ｷﾞ\" \"ギ\" #>   ..$ : chr [1:2] \"ｸﾞ\" \"グ\" #>   ..$ : chr [1:2] \"ｹﾞ\" \"ゲ\" #>   ..$ : chr [1:2] \"ｺﾞ\" \"ゴ\" #>   ..$ : chr [1:2] \"ｻﾞ\" \"ザ\" #>   ..$ : chr [1:2] \"ｼﾞ\" \"ジ\" #>   ..$ : chr [1:2] \"ｽﾞ\" \"ズ\" #>   ..$ : chr [1:2] \"ｾﾞ\" \"ゼ\" #>   ..$ : chr [1:2] \"ｿﾞ\" \"ゾ\" #>   ..$ : chr [1:2] \"ﾀﾞ\" \"ダ\" #>   ..$ : chr [1:2] \"ﾁﾞ\" \"ヂ\" #>   ..$ : chr [1:2] \"ﾂﾞ\" \"ヅ\" #>   ..$ : chr [1:2] \"ﾃﾞ\" \"デ\" #>   ..$ : chr [1:2] \"ﾄﾞ\" \"ド\" #>   ..$ : chr [1:2] \"ﾊﾞ\" \"バ\" #>   ..$ : chr [1:2] \"ﾋﾞ\" \"ビ\" #>   ..$ : chr [1:2] \"ﾌﾞ\" \"ブ\" #>   ..$ : chr [1:2] \"ﾍﾞ\" \"ベ\" #>   ..$ : chr [1:2] \"ﾎﾞ\" \"ボ\" #>   ..$ : chr [1:2] \"ﾊﾟ\" \"パ\" #>   ..$ : chr [1:2] \"ﾋﾟ\" \"ピ\" #>   ..$ : chr [1:2] \"ﾌﾟ\" \"プ\" #>   ..$ : chr [1:2] \"ﾍﾟ\" \"ペ\" #>   ..$ : chr [1:2] \"ﾎﾟ\" \"ポ\" #>   ..$ : chr [1:2] \"うﾞ\" \"ゔ\" #>   ..$ : chr [1:2] \"かﾞ\" \"が\" #>   ..$ : chr [1:2] \"きﾞ\" \"ぎ\" #>   ..$ : chr [1:2] \"くﾞ\" \"ぐ\" #>   ..$ : chr [1:2] \"けﾞ\" \"げ\" #>   ..$ : chr [1:2] \"こﾞ\" \"ご\" #>   ..$ : chr [1:2] \"さﾞ\" \"ざ\" #>   ..$ : chr [1:2] \"しﾞ\" \"じ\" #>   ..$ : chr [1:2] \"すﾞ\" \"ず\" #>   ..$ : chr [1:2] \"せﾞ\" \"ぜ\" #>   ..$ : chr [1:2] \"そﾞ\" \"ぞ\" #>   ..$ : chr [1:2] \"たﾞ\" \"だ\" #>   ..$ : chr [1:2] \"ちﾞ\" \"ぢ\" #>   ..$ : chr [1:2] \"つﾞ\" \"づ\" #>   ..$ : chr [1:2] \"てﾞ\" \"で\" #>   ..$ : chr [1:2] \"とﾞ\" \"ど\" #>   ..$ : chr [1:2] \"はﾞ\" \"ば\" #>   ..$ : chr [1:2] \"ひﾞ\" \"び\" #>   ..$ : chr [1:2] \"ふﾞ\" \"ぶ\" #>   ..$ : chr [1:2] \"へﾞ\" \"べ\" #>   ..$ : chr [1:2] \"ほﾞ\" \"ぼ\" #>   ..$ : chr [1:2] \"はﾟ\" \"ぱ\" #>   ..$ : chr [1:2] \"ひﾟ\" \"ぴ\" #>   ..$ : chr [1:2] \"ふﾟ\" \"ぷ\" #>   ..$ : chr [1:2] \"へﾟ\" \"ぺ\" #>   ..$ : chr [1:2] \"ほﾟ\" \"ぽ\" #>   ..$ : chr [1:2] \"ウﾞ\" \"ヴ\" #>   ..$ : chr [1:2] \"カﾞ\" \"ガ\" #>   ..$ : chr [1:2] \"キﾞ\" \"ギ\" #>   ..$ : chr [1:2] \"クﾞ\" \"グ\" #>   ..$ : chr [1:2] \"ケﾞ\" \"ゲ\" #>   ..$ : chr [1:2] \"コﾞ\" \"ゴ\" #>   ..$ : chr [1:2] \"サﾞ\" \"ザ\" #>   ..$ : chr [1:2] \"シﾞ\" \"ジ\" #>   ..$ : chr [1:2] \"スﾞ\" \"ズ\" #>   ..$ : chr [1:2] \"セﾞ\" \"ゼ\" #>   ..$ : chr [1:2] \"ソﾞ\" \"ゾ\" #>   ..$ : chr [1:2] \"タﾞ\" \"ダ\" #>   ..$ : chr [1:2] \"チﾞ\" \"ヂ\" #>   ..$ : chr [1:2] \"ツﾞ\" \"ヅ\" #>   ..$ : chr [1:2] \"テﾞ\" \"デ\" #>   ..$ : chr [1:2] \"トﾞ\" \"ド\" #>   ..$ : chr [1:2] \"ハﾞ\" \"バ\" #>   ..$ : chr [1:2] \"ヒﾞ\" \"ビ\" #>   ..$ : chr [1:2] \"フﾞ\" \"ブ\" #>   ..$ : chr [1:2] \"ヘﾞ\" \"ベ\" #>   ..$ : chr [1:2] \"ホﾞ\" \"ボ\" #>   ..$ : chr [1:2] \"ハﾟ\" \"パ\" #>   ..$ : chr [1:2] \"ヒﾟ\" \"ピ\" #>   ..$ : chr [1:2] \"フﾟ\" \"プ\" #>   ..$ : chr [1:2] \"ヘﾟ\" \"ペ\" #>   ..$ : chr [1:2] \"ホﾟ\" \"ポ\" #>   ..$ : chr [1:2] \"ゔ\" \"ゔ\" #>   ..$ : chr [1:2] \"が\" \"が\" #>   ..$ : chr [1:2] \"ぎ\" \"ぎ\" #>   ..$ : chr [1:2] \"ぐ\" \"ぐ\" #>   ..$ : chr [1:2] \"げ\" \"げ\" #>   ..$ : chr [1:2] \"ご\" \"ご\" #>   ..$ : chr [1:2] \"ざ\" \"ざ\" #>   ..$ : chr [1:2] \"じ\" \"じ\" #>   ..$ : chr [1:2] \"ず\" \"ず\" #>   ..$ : chr [1:2] \"ぜ\" \"ぜ\" #>   ..$ : chr [1:2] \"ぞ\" \"ぞ\" #>   ..$ : chr [1:2] \"だ\" \"だ\" #>   ..$ : chr [1:2] \"ぢ\" \"ぢ\" #>   ..$ : chr [1:2] \"づ\" \"づ\" #>   ..$ : chr [1:2] \"で\" \"で\" #>   ..$ : chr [1:2] \"ど\" \"ど\" #>   ..$ : chr [1:2] \"ば\" \"ば\" #>   ..$ : chr [1:2] \"び\" \"び\" #>   ..$ : chr [1:2] \"ぶ\" \"ぶ\" #>   ..$ : chr [1:2] \"べ\" \"べ\" #>   ..$ : chr [1:2] \"ぼ\" \"ぼ\" #>   .. [list output truncated]"},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill Japanese iteration marks — strj_fill_iter_mark","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"Fills Japanese iteration marks (Odori-ji) previous characters element 5 characters.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"","code":"strj_fill_iter_mark(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"","code":"strj_fill_iter_mark(c(   \"\\u3042\\u3044\\u3046\\u309d\\u3003\\u304b\\u304d\",   \"\\u91d1\\u5b50\\u307f\\u3059\\u309e\",   \"\\u306e\\u305f\\u308a\\u3033\\u3035\\u304b\\u306a\",   \"\\u3057\\u308d\\uff0f\\u2033\\uff3c\\u3068\\u3057\\u305f\" )) #> [1] \"あいうううかき\"  \"金子みすすﾞ\"     \"のたりたりかな\"  \"しろしﾞろとした\""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiraganize.html","id":null,"dir":"Reference","previous_headings":"","what":"Hiraganize Japanese characters — strj_hiraganize","title":"Hiraganize Japanese characters — strj_hiraganize","text":"Converts Japanese katakana hiragana. almost similar stringi::stri_trans_general(text, \"kana-hira\"), however, implementation can also handle additional symbols Japanese kana ligature (aka. goryaku-gana).","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiraganize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hiraganize Japanese characters — strj_hiraganize","text":"","code":"strj_hiraganize(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiraganize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hiraganize Japanese characters — strj_hiraganize","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiraganize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hiraganize Japanese characters — strj_hiraganize","text":"character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiraganize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hiraganize Japanese characters — strj_hiraganize","text":"","code":"strj_hiraganize(   c(     paste0(       \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",       \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",       \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"     ),     \"\\u677f\\u57a3\\u6b7b\\u30b9\\U0002a708\"   ) ) #> [1] \"あのいーはとーゔぉのすきとおった風\" \"板垣死すとも\""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":null,"dir":"Reference","previous_headings":"","what":"Katakanize Japanese characters — strj_katakanize","title":"Katakanize Japanese characters — strj_katakanize","text":"Converts Japanese hiragana katakana. almost similar stringi::stri_trans_general(text, \"hira-kana\"), however, implementation can also handle additional symbols Japanese kana ligature (aka. goryaku-gana).","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Katakanize Japanese characters — strj_katakanize","text":"","code":"strj_katakanize(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Katakanize Japanese characters — strj_katakanize","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Katakanize Japanese characters — strj_katakanize","text":"character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Katakanize Japanese characters — strj_katakanize","text":"","code":"strj_katakanize(   c(     paste0(       \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",       \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",       \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"     ),     \"\\u672c\\u65e5\\u309f\\u304b\\u304d\\u6c37\\u89e3\\u7981\"   ) ) #> [1] \"アノイーハトーヴォノスキトオッタ風\" \"本日ヨリカキ氷解禁\""},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert text following the rules of 'NEologd' — strj_normalize","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"Converts characters normalized style following rule recommended Neologism dictionary 'MeCab'.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"","code":"strj_normalize(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"text Character vector normalized.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"character vector.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"","code":"strj_normalize(   paste0(     \"\\u2015\\u2015\\u5357\\u30a2\\u30eb\\u30d7\\u30b9\",     \"\\u306e\\u3000\\u5929\\u7136\\u6c34-\\u3000\\uff33\",     \"\\uff50\\uff41\\uff52\\uff4b\\uff49\\uff4e\\uff47*\",     \"\\u3000\\uff2c\\uff45\\uff4d\\uff4f\\uff4e+\",     \"\\u3000\\u30ec\\u30e2\\u30f3\\u4e00\\u7d5e\\u308a\"   ) ) #> [1] \"ー南アルプスの天然水-Sparking* Lemon+レモン一絞り\""},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Rewrite text using rewrite.def — strj_rewrite_as_def","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"Rewrite text using rewrite.def","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"","code":"strj_rewrite_as_def(text, as = read_rewrite_def())"},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"text Character vector normalized. List.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"","code":"strj_rewrite_as_def(   paste0(     \"\\u2015\\u2015\\u5357\\u30a2\\u30eb\",     \"\\u30d7\\u30b9\\u306e\\u3000\\u5929\",     \"\\u7136\\u6c34-\\u3000\\uff33\\uff50\",     \"\\uff41\\uff52\\uff4b\\uff49\\uff4e\\uff47*\",     \"\\u3000\\uff2c\\uff45\\uff4d\\uff4f\\uff4e+\",     \"\\u3000\\u30ec\\u30e2\\u30f3\\u4e00\\u7d5e\\u308a\"   ) ) #> [1] \"――南アルプスの 天然水- Sparking* Lemon+ レモン一絞り\" strj_rewrite_as_def(   \"\\u60e1\\u3068\\u5047\\u9762\\u306e\\u30eb\\u30fc\\u30eb\",   read_rewrite_def(system.file(\"def/kyuji.def\", package = \"audubon\")) ) #> [1] \"悪と仮面のルール\""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":null,"dir":"Reference","previous_headings":"","what":"Romanize Japanese Hiragana and Katakana — strj_romanize","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"Romanize Japanese Hiragana Katakana","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"","code":"strj_romanize(   text,   config = c(\"wikipedia\", \"traditional hepburn\", \"modified hepburn\", \"kunrei\", \"nihon\") )"},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"text Character vector. elements composed except hiragana katakana letters, letters dropped return value. config Configuration used romanize. Default wikipedia.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"several ways romanize Japanese. Using implementation, can convert hiragana katakana 5 different styles; wikipedia style, traditional hepburn style, modified hepburn style, kunrei style, nihon style. Note styles return slightly different form stringi::stri_trans_general(text, \"-latn\").","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"","code":"strj_romanize(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ) ) #> [1] \"anoīhatōvonosukitōtta\""},{"path":"https://paithiov909.github.io/audubon/reference/strj_segment.html","id":null,"dir":"Reference","previous_headings":"","what":"Segment text into tokens — strj_segment","title":"Segment text into tokens — strj_segment","text":"alias strj_tokenize(engine = \"budoux\").","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_segment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Segment text into tokens — strj_segment","text":"","code":"strj_segment(text, format = c(\"list\", \"data.frame\"), split = FALSE)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_segment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Segment text into tokens — strj_segment","text":"text Character vector tokenized. format Output format. Choose list data.frame. split Logical. true, function splits vector sentences using stringi::stri_split_boundaries(type = \"sentence\") tokenizing.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_segment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Segment text into tokens — strj_segment","text":"List data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_segment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Segment text into tokens — strj_segment","text":"","code":"strj_segment(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ) ) #> $`1` #> [1] \"あのイーハトーヴォの\" \"すきと\"               \"おった\"               #> [4] \"風\"                   #>  strj_segment(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ),   format = \"data.frame\" ) #>   doc_id                token #> 1      1 あのイーハトーヴォの #> 2      1               すきと #> 3      1               おった #> 4      1                   風"},{"path":"https://paithiov909.github.io/audubon/reference/strj_tinyseg.html","id":null,"dir":"Reference","previous_headings":"","what":"Segment text into phrases — strj_tinyseg","title":"Segment text into phrases — strj_tinyseg","text":"alias strj_tokenize(engine = \"tinyseg\").","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tinyseg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Segment text into phrases — strj_tinyseg","text":"","code":"strj_tinyseg(text, format = c(\"list\", \"data.frame\"), split = FALSE)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_tinyseg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Segment text into phrases — strj_tinyseg","text":"text Character vector tokenized. format Output format. Choose list data.frame. split Logical. true, function splits vectors sentences using stringi::stri_split_boundaries(type = \"sentence\") tokenizing.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tinyseg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Segment text into phrases — strj_tinyseg","text":"list data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tinyseg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Segment text into phrases — strj_tinyseg","text":"","code":"strj_tinyseg(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ) ) #> $`1` #> [1] \"あ\"             \"の\"             \"イーハトーヴォ\" \"の\"             #> [5] \"すき\"           \"と\"             \"おっ\"           \"た\"             #> [9] \"風\"             #>  strj_tinyseg(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ),   format = \"data.frame\" ) #>   doc_id          token #> 1      1             あ #> 2      1             の #> 3      1 イーハトーヴォ #> 4      1             の #> 5      1           すき #> 6      1             と #> 7      1           おっ #> 8      1             た #> 9      1             風"},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Split text into tokens — strj_tokenize","title":"Split text into tokens — strj_tokenize","text":"Splits text several tokens using specified tokenizer.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split text into tokens — strj_tokenize","text":"","code":"strj_tokenize(   text,   format = c(\"list\", \"data.frame\"),   engine = c(\"stringi\", \"budoux\", \"tinyseg\", \"mecab\", \"sudachipy\"),   rcpath = NULL,   mode = c(\"C\", \"B\", \"A\"),   split = FALSE )"},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split text into tokens — strj_tokenize","text":"text Character vector tokenized. format Output format. Choose list data.frame. engine Tokenizer name. Choose one 'stringi', 'budoux', 'tinyseg', 'mecab', 'sudachipy'. Note specified tokenizer installed available use 'mecab' 'sudachipy'. rcpath Path setting file 'MeCab' 'sudachipy' . mode Splitting mode 'sudachipy'. split Logical. true, function splits vector sentences using stringi::stri_split_boundaries(type = \"sentence\") tokenizing.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split text into tokens — strj_tokenize","text":"list data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split text into tokens — strj_tokenize","text":"","code":"strj_tokenize(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ) ) #> $`1` #> [1] \"あの\"           \"イーハトーヴォ\" \"の\"             \"すき\"           #> [5] \"と\"             \"おっ\"           \"た\"             \"風\"             #>  strj_tokenize(   paste0(     \"\\u3042\\u306e\\u30a4\\u30fc\\u30cf\\u30c8\",     \"\\u30fc\\u30f4\\u30a9\\u306e\\u3059\\u304d\",     \"\\u3068\\u304a\\u3063\\u305f\\u98a8\"   ),   format = \"data.frame\" ) #>   doc_id          token #> 1      1           あの #> 2      1 イーハトーヴォ #> 3      1             の #> 4      1           すき #> 5      1             と #> 6      1           おっ #> 7      1             た #> 8      1             風"},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":null,"dir":"Reference","previous_headings":"","what":"Transcribe Arabic to Kansuji — strj_transcribe_num","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"Transcribes Arabic integers Kansuji auxiliary numerals.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"","code":"strj_transcribe_num(int)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"int Integers.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"implementation limited, function can transcribe numbers trillions. case convert much bigger numbers, try use 'arabic2kansuji' package.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"","code":"strj_transcribe_num(c(10L, 31415L)) #> [1] \"十\"             \"三万千四百十五\""},{"path":"https://paithiov909.github.io/audubon/news/index.html","id":"audubon-040","dir":"Changelog","previous_headings":"","what":"audubon 0.4.0","title":"audubon 0.4.0","text":"bind_tf_idf2 can calculate bind term frequency, inverse document frequency, tf-idf tidy text dataset. collapse_tokens mute_tokens, lexical_density can used handling tidy text dataset tokens. strj_tokenize now preserves original order text names. prettify now can get delim argument.","code":""},{"path":"https://paithiov909.github.io/audubon/news/index.html","id":"audubon-030","dir":"Changelog","previous_headings":"","what":"audubon 0.3.0","title":"audubon 0.3.0","text":"CRAN release: 2022-07-22 strj_fill_iter_mark now replaces sequence iteration marks recursively. strj_tokenize now can retrieve engine argument switch tokenizers splitting text tokens.","code":""},{"path":"https://paithiov909.github.io/audubon/news/index.html","id":"audubon-020","dir":"Changelog","previous_headings":"","what":"audubon 0.2.0","title":"audubon 0.2.0","text":"CRAN release: 2022-05-24 Updated ngram_tokenizer function. Added wrapper function ‘TinySegmenter’ written Taku Kudo.","code":""},{"path":"https://paithiov909.github.io/audubon/news/index.html","id":"audubon-012","dir":"Changelog","previous_headings":"","what":"audubon 0.1.2","title":"audubon 0.1.2","text":"CRAN release: 2022-04-02 Switched arguments order pack function. pack now accepts pull second argument n third argument. pull now can accept symbol.","code":""},{"path":"https://paithiov909.github.io/audubon/news/index.html","id":"audubon-011","dir":"Changelog","previous_headings":"","what":"audubon 0.1.1","title":"audubon 0.1.1","text":"CRAN release: 2022-02-14 Updated documentation.","code":""},{"path":"https://paithiov909.github.io/audubon/news/index.html","id":"audubon-010","dir":"Changelog","previous_headings":"","what":"audubon 0.1.0","title":"audubon 0.1.0","text":"Relicensed Apache License, Version 2.0. Added NEWS.md file track changes package.","code":""}]
