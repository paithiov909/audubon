[{"path":"https://paithiov909.github.io/audubon/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Akiru Kato. Maintainer.","code":""},{"path":"https://paithiov909.github.io/audubon/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kato (2021). audubon: R Package Various Japanese Text Processing. R package version 0.0.3.","code":"@Manual{,   title = {audubon: R Package for Various Japanese Text Processing},   author = {Akiru Kato},   year = {2021},   note = {R package version 0.0.3}, }"},{"path":"https://paithiov909.github.io/audubon/index.html","id":"audubon-","dir":"","previous_headings":"","what":"R Package for Various Japanese Text Processing","title":"R Package for Various Japanese Text Processing","text":"audubon R package various Japanese text processing contains: wrapper functions hakatashi/japanese.js R port SudachiCharNormalizer miscellaneous functions","code":""},{"path":"https://paithiov909.github.io/audubon/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"R Package for Various Japanese Text Processing","text":"","code":"remotes::install_github(\"paithio909/audubon\")"},{"path":"https://paithiov909.github.io/audubon/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"R Package for Various Japanese Text Processing","text":"MIT license. Icons made iconixar www.flaticon.com.","code":""},{"path":"https://paithiov909.github.io/audubon/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 audubon authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_as_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Pack prettified output as quanteda tokens — gbs_as_tokens","title":"Pack prettified output as quanteda tokens — gbs_as_tokens","text":"Pack prettified output quanteda tokens","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_as_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pack prettified output as quanteda tokens — gbs_as_tokens","text":"","code":"gbs_as_tokens(df, pull = \"token\", n = 1L, sep = \"-\", what = \"fastestword\", ...)"},{"path":"https://paithiov909.github.io/audubon/reference/gbs_as_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pack prettified output as quanteda tokens — gbs_as_tokens","text":"df prettified data.frame tokens. pull Column packed text ngrams body. Default value token. n Integer internally passed ngrams tokenizer function created audubon::ngram_tokenizer() sep Character scalar internally used concatenator ngrams. Character scalar; tokenizer use quanteda::tokens(). ... arguments passed quanteda::tokens().","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_as_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pack prettified output as quanteda tokens — gbs_as_tokens","text":"quanteda 'token' class object.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/gbs_c.html","id":null,"dir":"Reference","previous_headings":"","what":"An alternative of RMeCabC — gbs_c","title":"An alternative of RMeCabC — gbs_c","text":"alternative RMeCabC","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_c.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An alternative of RMeCabC — gbs_c","text":"","code":"gbs_c(df, pull = c(\"token\", \"Original\"), names = \"POS1\")"},{"path":"https://paithiov909.github.io/audubon/reference/gbs_c.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An alternative of RMeCabC — gbs_c","text":"df prettified data.frame tokenized sentences. pull column name df. names column name df.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_c.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An alternative of RMeCabC — gbs_c","text":"list named vectors.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/gbs_dfm.html","id":null,"dir":"Reference","previous_headings":"","what":"An alternative of docDF family — gbs_dfm","title":"An alternative of docDF family — gbs_dfm","text":"Create sparse document-feature matrix. function shorthand audubon::gbs_as_tokens() quatenda::dfm().","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_dfm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An alternative of docDF family — gbs_dfm","text":"","code":"gbs_dfm(df, pull = \"token\", n = 1L, sep = \"-\", what = \"fastestword\", ...)"},{"path":"https://paithiov909.github.io/audubon/reference/gbs_dfm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An alternative of docDF family — gbs_dfm","text":"df prettified data.frame tokens. pull Column packed text ngrams body. Default value token. n Integer internally passed ngrams tokenizer function created audubon::ngram_tokenizer() sep Character scalar internally used concatenator ngrams. Character scalar; tokenizer use quanteda::tokens(). ... arguments passed quanteda::tokens().","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_dfm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An alternative of docDF family — gbs_dfm","text":"quanteda 'dfm' object.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/gbs_freq.html","id":null,"dir":"Reference","previous_headings":"","what":"An alternative of RMeCabFreq — gbs_freq","title":"An alternative of RMeCabFreq — gbs_freq","text":"alternative RMeCabFreq","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_freq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An alternative of RMeCabFreq — gbs_freq","text":"","code":"gbs_freq(df, ..., .name_repair = TRUE)"},{"path":"https://paithiov909.github.io/audubon/reference/gbs_freq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An alternative of RMeCabFreq — gbs_freq","text":"df prettified data.frame tokenized sentences. ... arguments passed dplyr::tally(). .name_repair Logical: true, rename column names RMeCabFreq-compatible style.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/gbs_freq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An alternative of RMeCabFreq — gbs_freq","text":"data.frame.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/gibasa.html","id":null,"dir":"Reference","previous_headings":"","what":"Gibasa functions family — gbs","title":"Gibasa functions family — gbs","text":"functions compact RMeCab alternatives based quanteda.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Ngrams tokenizer — ngram_tokenizer","title":"Ngrams tokenizer — ngram_tokenizer","text":"Make ngram tokenizer function.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ngrams tokenizer — ngram_tokenizer","text":"","code":"ngram_tokenizer(n = 1L, skip_word_none = FALSE, locale = NULL)"},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ngrams tokenizer — ngram_tokenizer","text":"n Integer. skip_word_none Logical. locale Character scalar. Pass NULL empty string default locale.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/ngram_tokenizer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ngrams tokenizer — ngram_tokenizer","text":"ngram tokenizer function","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":null,"dir":"Reference","previous_headings":"","what":"Pack prettified data.frame of tokens — pack","title":"Pack prettified data.frame of tokens — pack","text":"Pack prettified data.frame tokens new data.frame corpus, compatible Text Interchange Formats.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pack prettified data.frame of tokens — pack","text":"","code":"pack(df, n = 1L, pull = \"token\", sep = \"-\", .collapse = \" \")"},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pack prettified data.frame of tokens — pack","text":"df prettified data.frame tokens. n Integer internally passed ngrams tokenizer function created audubon::ngram_tokenizer() pull Column packed text ngrams body. Default value token. sep Character scalar internally used concatenator ngrams. .collapse argument passed stringi::stri_join().","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pack prettified data.frame of tokens — pack","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"text-interchange-formats-tif-","dir":"Reference","previous_headings":"","what":"Text Interchange Formats (TIF)","title":"Pack prettified data.frame of tokens — pack","text":"Text Interchange Formats (TIF) set standards allows R text analysis packages target defined inputs outputs corpora, tokens, document-term matrices.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pack.html","id":"valid-data-frame-of-tokens","dir":"Reference","previous_headings":"","what":"Valid data.frame of tokens","title":"Pack prettified data.frame of tokens — pack","text":"prettified data.frame tokens data.frame object compatible TIF. TIF valid data.frame tokens expected one unique key column (named doc_id) text several feature columns tokens. feature columns must contain least token .","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://paithiov909.github.io/audubon/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Read rewrite.def file — read_rewrite_def","title":"Read rewrite.def file — read_rewrite_def","text":"Read rewrite.def file","code":""},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read rewrite.def file — read_rewrite_def","text":"","code":"read_rewrite_def(   def_path = system.file(\"def/rewrite.def\", package = \"audubon\") )"},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read rewrite.def file — read_rewrite_def","text":"def_path Character scalar; path rewriting definition file.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/read_rewrite_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read rewrite.def file — read_rewrite_def","text":"List.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill Japanese iteration marks — strj_fill_iter_mark","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"Fill Japanese iteration marks (Odoriji) previous characters.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"","code":"strj_fill_iter_mark(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_fill_iter_mark.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill Japanese iteration marks — strj_fill_iter_mark","text":"Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiranganize.html","id":null,"dir":"Reference","previous_headings":"","what":"Hiraganize Japanese characters — strj_hiranganize","title":"Hiraganize Japanese characters — strj_hiranganize","text":"Hiraganize Japanese characters","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiranganize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hiraganize Japanese characters — strj_hiranganize","text":"","code":"strj_hiranganize(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiranganize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hiraganize Japanese characters — strj_hiranganize","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_hiranganize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hiraganize Japanese characters — strj_hiranganize","text":"Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":null,"dir":"Reference","previous_headings":"","what":"Katakanize Japanese characters — strj_katakanize","title":"Katakanize Japanese characters — strj_katakanize","text":"Katakanize Japanese characters","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Katakanize Japanese characters — strj_katakanize","text":"","code":"strj_katakanize(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Katakanize Japanese characters — strj_katakanize","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_katakanize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Katakanize Japanese characters — strj_katakanize","text":"Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert text following the rules of 'NEologd' — strj_normalize","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"Convert characters normalized style basing rules recommended Neologism dictionary MeCab.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"","code":"strj_normalize(text)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"text Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert text following the rules of 'NEologd' — strj_normalize","text":"Character vector.","code":""},{"path":[]},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Rewrite text using rewrite.def — strj_rewrite_as_def","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"Rewrite text using rewrite.def","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"","code":"strj_rewrite_as_def(text, as = read_rewrite_def())"},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"text Character vector. List.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_rewrite_as_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rewrite text using rewrite.def — strj_rewrite_as_def","text":"Character vector","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":null,"dir":"Reference","previous_headings":"","what":"Romanize Japanese Hiragana and Katakana — strj_romanize","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"Romanize Japanese Hiragana Katakana","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"","code":"strj_romanize(   text,   config = c(\"wikipedia\", \"traditional hepburn\", \"modified hepburn\", \"kunrei\", \"nihon\") )"},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"text Character vector. config Config used romanize.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_romanize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Romanize Japanese Hiragana and Katakana — strj_romanize","text":"Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Simply tokenize sentence — strj_tokenize","title":"Simply tokenize sentence — strj_tokenize","text":"Split given sentence tokens using stringi::stri_split_boundaries(type = \"word\").","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simply tokenize sentence — strj_tokenize","text":"","code":"strj_tokenize(sentence)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simply tokenize sentence — strj_tokenize","text":"sentence Character vector tokenized.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simply tokenize sentence — strj_tokenize","text":"data.frame.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":null,"dir":"Reference","previous_headings":"","what":"Transcribe Arabic to Kansuji — strj_transcribe_num","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"Transcribe Arabic integers Kansuji.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"","code":"strj_transcribe_num(int)"},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"int Integers.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/strj_transcribe_num.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transcribe Arabic to Kansuji — strj_transcribe_num","text":"Character vector.","code":""},{"path":"https://paithiov909.github.io/audubon/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"page lists tidy eval tools reexported package rlang. learn using tidy eval scripts packages high level, see dplyr programming vignette ggplot2 packages vignette. Metaprogramming section Advanced R may also useful deeper dive. tidy eval operators {{, !!, !!! syntactic constructs specially interpreted tidy eval functions. mostly need {{, !! !!! advanced operators use simple cases. curly-curly operator {{ allows tunnel data-variables passed function arguments inside tidy eval functions. {{ designed individual arguments. pass multiple arguments contained dots, use ... normal way. enquo() enquos() delay execution one several function arguments. former returns single expression, latter returns list expressions. defused, expressions longer evaluate . must injected back evaluation context !! (single expression) !!! (list expressions). simple case, code equivalent usage {{ ... . Defusing enquo() enquos() needed complex cases, instance need inspect modify expressions way. .data pronoun object represents current slice data. variable name string, use .data pronoun subset variable [[. Another tidy eval operator :=. makes possible use glue curly-curly syntax LHS =. technical reasons, R language support complex expressions left =, use := workaround. Many tidy eval functions like dplyr::mutate() dplyr::summarise() give automatic name unnamed inputs. need create sort automatic names , use as_label(). instance, glue-tunnelling syntax can reproduced manually : Expressions defused enquo() (tunnelled {{) need simple column names, can arbitrarily complex. as_label() handles cases gracefully. code assumes simple column name, use as_name() instead. safer throws error input name expected.","code":"my_function <- function(data, var, ...) {   data %>%     group_by(...) %>%     summarise(mean = mean({{ var }})) } my_function <- function(data, var, ...) {   # Defuse   var <- enquo(var)   dots <- enquos(...)    # Inject   data %>%     group_by(!!!dots) %>%     summarise(mean = mean(!!var)) } my_var <- \"disp\" mtcars %>% summarise(mean = mean(.data[[my_var]])) my_function <- function(data, var, suffix = \"foo\") {   # Use `{{` to tunnel function arguments and the usual glue   # operator `{` to interpolate plain strings.   data %>%     summarise(\"{{ var }}_mean_{suffix}\" := mean({{ var }})) } my_function <- function(data, var, suffix = \"foo\") {   var <- enquo(var)   prefix <- as_label(var)   data %>%     summarise(\"{prefix}_mean_{suffix}\" := mean(!!var)) }"}]
